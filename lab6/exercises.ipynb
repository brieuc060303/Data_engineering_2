{"cells":[{"cell_type":"markdown","id":"ba450fb1-8a26-4894-ab7a-5d7bfefe90ce","metadata":{"id":"ba450fb1-8a26-4894-ab7a-5d7bfefe90ce"},"source":["# Chapter 6 - Exercises\n","\n","> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n","\n","---\n"]},{"cell_type":"markdown","id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e","metadata":{"id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e"},"source":["## Exercise 6.1: Increasing the context length"]},{"cell_type":"markdown","id":"3c1b5925","metadata":{"id":"3c1b5925"},"source":["**Padding Input Sequences in Neural Language Models**\n","\n","**Key Research Question: How does padding inputs to the maximum `token` length affect model predictive performance?**\n","\n","*Methodological Approach:*\n","- Implement systematic `token` padding\n","- Analyze padding's impact on model performance\n","- Explore input representation interactions\n","\n","*Critical Parameters:*\n","- Input `padding` strategy\n","- Maximum `token` length\n","- Predictive performance metrics\n","\n","*Recommended Investigation:*\n","1. Implement maximum-length input `padding`\n","2. Measure performance variations\n","3. Compare padded versus non-padded inputs\n","4. Assess computational implications"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/badr/lab6')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQ2S4ExEJmsL","executionInfo":{"status":"ok","timestamp":1737545885176,"user_tz":-60,"elapsed":1413,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"007f6f8f-52fd-4030-816d-b657bb29964a"},"id":"jQ2S4ExEJmsL","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from gpt_class_finetune import *"],"metadata":{"id":"iMH4v0MGKsx4","executionInfo":{"status":"ok","timestamp":1737545902013,"user_tz":-60,"elapsed":16838,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}}},"id":"iMH4v0MGKsx4","execution_count":6,"outputs":[]},{"cell_type":"code","source":["import urllib.request\n","import zipfile\n","import os\n","from pathlib import Path\n","import time\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tiktoken\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from gpt_download import download_and_load_gpt2\n","from previous_labs import GPTModel, load_weights_into_gpt\n","\n","import argparse"],"metadata":{"id":"59Yiwmh0LU3A","executionInfo":{"status":"ok","timestamp":1737545902014,"user_tz":-60,"elapsed":3,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}}},"id":"59Yiwmh0LU3A","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["We can pad the inputs by setting the max_length to 1024"],"metadata":{"id":"SSoEPsawcRQd"},"id":"SSoEPsawcRQd"},{"cell_type":"code","source":["max_length = 1024"],"metadata":{"id":"4iwXg0mUcIk9","executionInfo":{"status":"ok","timestamp":1737545902014,"user_tz":-60,"elapsed":3,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}}},"id":"4iwXg0mUcIk9","execution_count":8,"outputs":[]},{"cell_type":"markdown","id":"5a780455-f52a-48d1-ab82-6afd40bcad8b","metadata":{"id":"5a780455-f52a-48d1-ab82-6afd40bcad8b"},"source":["## Exercise 6.2: Finetuning the whole model"]},{"cell_type":"markdown","id":"1e23d900","metadata":{"id":"1e23d900"},"source":["**Model-Wide Fine-Tuning Performance Assessment**\n","\n","**Key Research Question: What is the impact of `fine-tuning` the entire transformer model versus a single final block on predictive performance?**\n","\n","\n","*Methodological Approach:*\n","- Implement comprehensive model `fine-tuning`\n","- Compare performance against single block tuning\n","- Assess computational and representational changes\n","\n","*Critical Parameters:*\n","- Full model `fine-tuning` strategy\n","- Performance evaluation metrics\n","- Comparative analysis methodology\n","\n","*Recommended Investigation:*\n","1. `Fine-tune` entire transformer model\n","2. Measure predictive performance metrics\n","3. Compare with previous single-block tuning results\n","4. Analyze performance variation mechanisms"]},{"cell_type":"markdown","source":["We need to remove the lines to finetune the whole model :\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n"],"metadata":{"id":"wWdhG9Fmc3Ot"},"id":"wWdhG9Fmc3Ot"},{"cell_type":"code","source":["\n","    ########################################\n","    # Download and prepare dataset\n","    ########################################\n","\n","url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n","zip_path = \"sms_spam_collection.zip\"\n","extracted_path = \"sms_spam_collection\"\n","data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n","\n","download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n","df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n","balanced_df = create_balanced_dataset(df)\n","balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n","\n","train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n","train_df.to_csv(\"train.csv\", index=None)\n","validation_df.to_csv(\"validation.csv\", index=None)\n","test_df.to_csv(\"test.csv\", index=None)\n","\n","    ########################################\n","    # Create data loaders\n","    ########################################\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","train_dataset = SpamDataset(\n","        csv_file=\"train.csv\",\n","        max_length=None,\n","        tokenizer=tokenizer\n","    )\n","\n","val_dataset = SpamDataset(\n","        csv_file=\"validation.csv\",\n","        max_length=train_dataset.max_length,\n","        tokenizer=tokenizer\n","    )\n","\n","test_dataset = SpamDataset(\n","        csv_file=\"test.csv\",\n","        max_length=train_dataset.max_length,\n","        tokenizer=tokenizer\n","    )\n","\n","num_workers = 0\n","batch_size = 8\n","\n","torch.manual_seed(123)\n","\n","train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        drop_last=True,\n","    )\n","\n","val_loader = DataLoader(\n","        dataset=val_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        drop_last=False,\n","    )\n","\n","test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        drop_last=False,\n","    )\n","\n","    ########################################\n","    # Load pretrained model\n","    ########################################\n","\n","    # Small GPT model for testing purposes\n","\n","\n","CHOOSE_MODEL = \"gpt2-small (124M)\"\n","INPUT_PROMPT = \"Every effort moves\"\n","\n","BASE_CONFIG = {\n","            \"vocab_size\": 50257,     # Vocabulary size\n","            \"context_length\": 1024,  # Context length\n","            \"drop_rate\": 0.0,        # Dropout rate\n","            \"qkv_bias\": True         # Query-key-value bias\n","        }\n","\n","model_configs = {\n","            \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","            \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","            \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","            \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","        }\n","\n","BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n","\n","assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n","            f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n","            f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n","            f\"`max_length={BASE_CONFIG['context_length']}`\"\n","        )\n","\n","model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n","settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n","\n","model = GPTModel(BASE_CONFIG)\n","load_weights_into_gpt(model, params)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    ########################################\n","    # Modify and pretrained model\n","    ########################################\n","\n","\n","torch.manual_seed(123)\n","\n","num_classes = 2\n","model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n","model.to(device)\n","\n","for param in model.trf_blocks[-1].parameters():\n","        param.requires_grad = True\n","\n","for param in model.final_norm.parameters():\n","        param.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gb52dkdWLeo9","executionInfo":{"status":"ok","timestamp":1737545919060,"user_tz":-60,"elapsed":17049,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"901d0e0e-8c73-4c0a-f994-81b66fd98c3a"},"id":"Gb52dkdWLeo9","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"]},{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 160kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 6.56MiB/s]\n","hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 69.4kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:11<00:00, 44.5MiB/s]\n","model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 9.64MiB/s]\n","model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 2.47MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.73MiB/s]\n"]}]},{"cell_type":"code","source":["with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n","\n","print(f\"Training loss: {train_loss:.3f}\")\n","print(f\"Validation loss: {val_loss:.3f}\")\n","print(f\"Test loss: {test_loss:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9M0v62UMwE9","executionInfo":{"status":"ok","timestamp":1737545990488,"user_tz":-60,"elapsed":71431,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"4f353850-19ad-4c04-90b1-959c87c3d690"},"id":"Q9M0v62UMwE9","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 2.183\n","Validation loss: 2.583\n","Test loss: 2.322\n"]}]},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n","\n","num_epochs = 5\n","train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=50, eval_iter=5,tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh0LCjpBM_Lx","executionInfo":{"status":"ok","timestamp":1737554509696,"user_tz":-60,"elapsed":8519211,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"dc3e8a90-ca3d-467f-90cc-e0cec7db2e74"},"id":"Yh0LCjpBM_Lx","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 2.884, Val loss 2.596\n","Ep 1 (Step 000050): Train loss 0.293, Val loss 0.190\n","Ep 1 (Step 000100): Train loss 0.148, Val loss 0.501\n","Training accuracy: 97.50% | Validation accuracy: 95.00%\n","Ep 2 (Step 000150): Train loss 0.162, Val loss 0.073\n","Ep 2 (Step 000200): Train loss 0.004, Val loss 0.029\n","Ep 2 (Step 000250): Train loss 0.029, Val loss 0.106\n","Training accuracy: 97.50% | Validation accuracy: 95.00%\n","Ep 3 (Step 000300): Train loss 0.010, Val loss 0.127\n","Ep 3 (Step 000350): Train loss 0.001, Val loss 0.009\n","Training accuracy: 100.00% | Validation accuracy: 100.00%\n","Ep 4 (Step 000400): Train loss 0.005, Val loss 0.006\n","Ep 4 (Step 000450): Train loss 0.013, Val loss 0.026\n","Ep 4 (Step 000500): Train loss 0.003, Val loss 0.112\n","Training accuracy: 100.00% | Validation accuracy: 97.50%\n","Ep 5 (Step 000550): Train loss 0.025, Val loss 0.024\n","Ep 5 (Step 000600): Train loss 0.001, Val loss 0.044\n","Training accuracy: 100.00% | Validation accuracy: 100.00%\n","Training completed in 141.98 minutes.\n"]}]},{"cell_type":"code","source":["with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n","\n","print(f\"Training loss: {train_loss:.3f}\")\n","print(f\"Validation loss: {val_loss:.3f}\")\n","print(f\"Test loss: {test_loss:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fl-_HiIRNJP2","executionInfo":{"status":"ok","timestamp":1737554566139,"user_tz":-60,"elapsed":56456,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"6b5c3113-c83b-4675-b068-3bec777a1cb7"},"id":"Fl-_HiIRNJP2","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 0.003\n","Validation loss: 0.004\n","Test loss: 0.605\n"]}]},{"cell_type":"markdown","id":"2269bce3-f2b5-4a76-a692-5977c75a57b6","metadata":{"id":"2269bce3-f2b5-4a76-a692-5977c75a57b6"},"source":["## Exercise 6.3: Finetuning the first versus last token"]},{"cell_type":"markdown","id":"09ddba60","metadata":{"id":"09ddba60"},"source":["**First Token Fine-Tuning: Predictive Performance Analysis**\n","\n","**Key Research Question: How do predictive performance characteristics change when fine-tuning the first output `token` compared to the last output `token`?**\n","\n","*Methodological Approach:*\n","- Fine-tune first output `token`\n","- Compare performance against last `token` fine-tuning\n","- Assess representational learning variations\n","\n","*Critical Parameters:*\n","- Initial `token` fine-tuning strategy\n","- Performance evaluation metrics\n","- Comparative analysis methodology\n","\n","*Recommended Investigation:*\n","1. Implement first `token` fine-tuning\n","2. Measure predictive performance\n","3. Compare with last `token` fine-tuning results\n","4. Analyze performance variation mechanisms"]},{"cell_type":"code","source":["def calc_loss_batch_first_token(input_batch, target_batch, model, device):\n","    \"\"\"\n","    Compute the loss for a batch, focusing on the first token in the sequence.\n","\n","    Args:\n","        input_batch: Tensor of input sequences.\n","        target_batch: Tensor of target labels.\n","        model: The language model.\n","        device: The computation device (CPU or GPU).\n","\n","    Returns:\n","        The computed loss for the batch.\n","    \"\"\"\n","    input_batch = input_batch.to(device)\n","    target_batch = target_batch.to(device)\n","\n","    # Forward pass\n","    outputs = model(input_batch)\n","\n","    # Extract logits for the first token\n","    first_token_logits = outputs[:, 0, :]  # Logits corresponding to the first token\n","\n","    # Define the loss function (e.g., CrossEntropyLoss)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Compute the loss between predicted logits and target labels\n","    loss = criterion(first_token_logits, target_batch)\n","\n","    return loss\n","\n","def calc_accuracy_loader_first_token(loader, model, device, num_batches=None):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for input_batch, target_batch in loader:\n","            if num_batches and total >= num_batches:\n","                break\n","\n","            input_batch = input_batch.to(device)\n","            target_batch = target_batch.to(device)\n","\n","            outputs = model(input_batch)\n","            first_token_logits = outputs[:, 0, :]  # Using index 0 for the first token\n","\n","            first_token_labels = target_batch\n","\n","            # Calculate the predictions and compare with the labels\n","            _, predicted = torch.max(first_token_logits, 1)\n","            correct += (predicted == first_token_labels).sum().item()\n","            total += target_batch.shape[0]\n","\n","    return correct / total if total > 0 else 0\n","\n","\n","\n","def evaluate_model_first_token(model, train_loader, val_loader, device, num_batches=None):\n","    model.eval()\n","    train_loss = calc_loss_loader_first_token(train_loader, model, device, num_batches)\n","    val_loss = calc_loss_loader_first_token(val_loader, model, device, num_batches)\n","    return train_loss, val_loss\n","\n","def calc_loss_loader_first_token(loader, model, device, num_batches=None):\n","    model.eval()\n","    total_loss = 0\n","    batch_count = 0\n","\n","    with torch.no_grad():\n","        for input_batch, target_batch in loader:\n","            if num_batches and batch_count >= num_batches:\n","                break\n","\n","            loss = calc_loss_batch_first_token(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","            batch_count += 1\n","\n","    return total_loss / batch_count if batch_count > 0 else float('inf')\n","\n","def calc_accuracy_loader_first_token(loader, model, device, num_batches=None):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for input_batch, target_batch in loader:\n","            if num_batches and total >= num_batches:\n","                break\n","\n","            input_batch = input_batch.to(device)\n","            target_batch = target_batch.to(device)\n","\n","            outputs = model(input_batch)\n","            first_token_logits = outputs[:, 0, :]  # Using index 0 for the first token\n","\n","            first_token_labels = target_batch\n","\n","            # Calculate the predictions and compare with the labels\n","            _, predicted = torch.max(first_token_logits, 1)\n","            correct += (predicted == first_token_labels).sum().item()\n","            total += target_batch.shape[0]\n","\n","    return correct / total if total > 0 else 0\n","\n","\n","def train_classifier_first_token(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n","    examples_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","\n","            loss = calc_loss_batch_first_token(input_batch, target_batch, model, device)\n","            loss.backward()  # Calculate loss gradients\n","            optimizer.step()\n","\n","            examples_seen += input_batch.shape[0]\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model_first_token(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","                # Calculate accuracy after each evaluation step\n","                train_accuracy = calc_accuracy_loader_first_token(\n","                    train_loader, model, device, num_batches=eval_iter)\n","                val_accuracy = calc_accuracy_loader_first_token(\n","                    val_loader, model, device, num_batches=eval_iter)\n","\n","                print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n","                print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n","\n","                train_accs.append(train_accuracy)\n","                val_accs.append(val_accuracy)\n","\n","    return train_losses, val_losses, train_accs, val_accs, examples_seen"],"metadata":{"id":"onB9LzvUVnOU","executionInfo":{"status":"ok","timestamp":1737554822701,"user_tz":-60,"elapsed":195,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}}},"id":"onB9LzvUVnOU","execution_count":15,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","\n","torch.manual_seed(123)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n","\n","num_epochs = 5\n","train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_first_token(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=50, eval_iter=5,tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vaQfE9hIWkS2","executionInfo":{"status":"ok","timestamp":1737563310073,"user_tz":-60,"elapsed":8483728,"user":{"displayName":"Brieuc VALENCE","userId":"16754933697826839051"}},"outputId":"454e8016-412f-4b9f-afb7-7d88d2e6b1a7"},"id":"vaQfE9hIWkS2","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 0.931, Val loss 0.942\n","Training accuracy: 62.50% | Validation accuracy: 50.00%\n","Ep 1 (Step 000050): Train loss 0.584, Val loss 0.654\n","Training accuracy: 100.00% | Validation accuracy: 62.50%\n","Ep 1 (Step 000100): Train loss 0.589, Val loss 0.762\n","Training accuracy: 75.00% | Validation accuracy: 62.50%\n","Ep 2 (Step 000150): Train loss 0.436, Val loss 0.677\n","Training accuracy: 87.50% | Validation accuracy: 50.00%\n","Ep 2 (Step 000200): Train loss 0.435, Val loss 0.679\n","Training accuracy: 87.50% | Validation accuracy: 62.50%\n","Ep 2 (Step 000250): Train loss 0.354, Val loss 0.685\n","Training accuracy: 87.50% | Validation accuracy: 62.50%\n","Ep 3 (Step 000300): Train loss 0.303, Val loss 0.887\n","Training accuracy: 87.50% | Validation accuracy: 50.00%\n","Ep 3 (Step 000350): Train loss 0.344, Val loss 0.822\n","Training accuracy: 100.00% | Validation accuracy: 62.50%\n","Ep 4 (Step 000400): Train loss 0.220, Val loss 0.867\n","Training accuracy: 75.00% | Validation accuracy: 37.50%\n","Ep 4 (Step 000450): Train loss 0.190, Val loss 0.856\n","Training accuracy: 100.00% | Validation accuracy: 50.00%\n","Ep 4 (Step 000500): Train loss 0.244, Val loss 0.859\n","Training accuracy: 87.50% | Validation accuracy: 50.00%\n","Ep 5 (Step 000550): Train loss 0.254, Val loss 1.500\n","Training accuracy: 87.50% | Validation accuracy: 50.00%\n","Ep 5 (Step 000600): Train loss 0.236, Val loss 1.225\n","Training accuracy: 87.50% | Validation accuracy: 62.50%\n","Training completed in 141.39 minutes.\n"]}]},{"cell_type":"markdown","source":["En ayant changé model(input_batch)[:, -1, :] en model(input_batch)[:, 0, :], on a pu avoir le premier token au lieu du dernier. On voit que le fine tunning token forward est moins efficace que le back."],"metadata":{"id":"sLlh7o1Pw4GP"},"id":"sLlh7o1Pw4GP"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}