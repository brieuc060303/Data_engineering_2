{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/projet')\n","import os\n","os.chdir('/content/drive/MyDrive/projet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ostupSlrjF6P","executionInfo":{"status":"ok","timestamp":1737988584461,"user_tz":-60,"elapsed":27082,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"outputId":"77b10de8-4783-48ba-96cb-8bd780b58682"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CPJpNKJj7AW","executionInfo":{"status":"ok","timestamp":1737988588104,"user_tz":-60,"elapsed":3646,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"outputId":"5704943a-a2b1-4301-cd24-e4150c914bdf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n","Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.0/1.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"E7sNKYyZi_h4","executionInfo":{"status":"ok","timestamp":1737988602699,"user_tz":-60,"elapsed":14598,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from gpt_download import download_and_load_gpt2\n","from previous_labs import (\n","    create_dataloader_v1,\n","    calc_loss_loader,\n","    generate,\n","    GPTModel,\n","    load_weights_into_gpt,\n","    text_to_token_ids,\n","    train_model_simple,\n","    token_ids_to_text,\n",")\n","import tiktoken\n","import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmIpD7Zli_iC","outputId":"bf3ffd12-e95f-4489-fe66-4e2d35c49b05","executionInfo":{"status":"ok","timestamp":1737988727563,"user_tz":-60,"elapsed":124866,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["File already exists and is up-to-date: gpt2/124M/checkpoint\n","File already exists and is up-to-date: gpt2/124M/encoder.json\n","File already exists and is up-to-date: gpt2/124M/hparams.json\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n","File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.cuda.empty_cache()\n","GPT_124M = {\n","        \"vocab_size\": 50257,     # Vocabulary size\n","        \"context_length\": 1024,  # Context length\n","        \"drop_rate\": 0.0,        # Dropout rate\n","        \"qkv_bias\": True,         # Query-key-value bias\n","        \"emb_dim\": 768,\n","        \"n_layers\": 12,\n","        \"n_heads\": 12\n","    }\n","\n","model_size = '124M'\n","settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n","model = GPTModel(GPT_124M)\n","load_weights_into_gpt(model, params)\n","model.eval()\n","model.to(device)\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5w8LhEAgi_iE","executionInfo":{"status":"ok","timestamp":1737988730572,"user_tz":-60,"elapsed":3018,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[],"source":["!cd /content/drive/MyDrive/projet\n","df = pd.read_json(\"final_dataset_clean.json\")\n","df.shape\n","df = df.sample(frac=0.5, random_state=42)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MfnKJp8Ui_iF","executionInfo":{"status":"ok","timestamp":1737988730573,"user_tz":-60,"elapsed":6,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6afed170-ffc7-400e-dbfc-b625f6142662"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 22170 entries, 1162 to 31430\n","Data columns (total 2 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   instruction  22170 non-null  object\n"," 1   response     22170 non-null  object\n","dtypes: object(2)\n","memory usage: 519.6+ KB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"kdVrgxpii_iF"},"source":["I choosed a finance dataset, who can help me in everyday questions on what is safe to do on financial subject.\n","Dataset found on Hugging face, https://huggingface.co/datasets/gbharti/wealth-alpaca_lora"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"xqy3-qP2i_iH","executionInfo":{"status":"ok","timestamp":1737988730573,"user_tz":-60,"elapsed":4,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[],"source":["#Refine into <instruction, response>, the dataset is already using Alpaca-style\n","df = df.rename(columns={'output': 'response'})"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uJK7W-YNi_iI","executionInfo":{"status":"ok","timestamp":1737988731080,"user_tz":-60,"elapsed":510,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"32bccb59-1975-4bf7-f251-8ad548b62173"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             instruction  \\\n","1162           When should I walk away from my mortgage?   \n","2279   What makes a Company's Stock prices go up or d...   \n","42511           Describe the key features of a REST API.   \n","40674  Generate a response that conveys an appropriat...   \n","26379          Name three sciences related to computing.   \n","\n","                                                response  \n","1162   This is a very personal situation of course, b...  \n","2279   I always liked the answer that in the short te...  \n","42511  A REST API is a type of API architecture that ...  \n","40674  I apologize for the misunderstanding. Is there...  \n","26379  Three sciences related to computing are comput...  "],"text/html":["\n","  <div id=\"df-4ed37ccb-c5f5-44a1-a772-62be61d61e28\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instruction</th>\n","      <th>response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1162</th>\n","      <td>When should I walk away from my mortgage?</td>\n","      <td>This is a very personal situation of course, b...</td>\n","    </tr>\n","    <tr>\n","      <th>2279</th>\n","      <td>What makes a Company's Stock prices go up or d...</td>\n","      <td>I always liked the answer that in the short te...</td>\n","    </tr>\n","    <tr>\n","      <th>42511</th>\n","      <td>Describe the key features of a REST API.</td>\n","      <td>A REST API is a type of API architecture that ...</td>\n","    </tr>\n","    <tr>\n","      <th>40674</th>\n","      <td>Generate a response that conveys an appropriat...</td>\n","      <td>I apologize for the misunderstanding. Is there...</td>\n","    </tr>\n","    <tr>\n","      <th>26379</th>\n","      <td>Name three sciences related to computing.</td>\n","      <td>Three sciences related to computing are comput...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ed37ccb-c5f5-44a1-a772-62be61d61e28')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4ed37ccb-c5f5-44a1-a772-62be61d61e28 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4ed37ccb-c5f5-44a1-a772-62be61d61e28');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ef5fdad5-eee5-4071-b1e5-3f08b0b5968a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef5fdad5-eee5-4071-b1e5-3f08b0b5968a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ef5fdad5-eee5-4071-b1e5-3f08b0b5968a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 22170,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17688,\n        \"samples\": [\n          \"Create a list of potential guest speakers for a conference\",\n          \"Classify the following observation into a primary level of biological organisation.\",\n          \"Describe a workflow that a customer service representative would use to handle an angry customer.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21956,\n        \"samples\": [\n          \"It is certainly legal to transfer money between people, no matter how often, as long as the money is not originally from illegal sources. If you are gaining in the process, you need to pay taxes on your (net) gain, as on any income; but as always, taxed income is still income. Consider the accumulating transaction cost, the inherent risk (of your friend keeping the money), and the risk of the exchange rate going the other way; but otherwise it is a simple arbitrage business. There are thousands of people who do that all year long at stock exchanges and money markets; you might be able to do it more efficient there, and you don't need a 'friend' on the other side for that.\",\n          \"Dollar cost averaging doesn't (or shouldn't) apply here. DCA is the natural way we invest in the market, buying in by a steady dollar amount each pay period, so over time we can buy more shares when the market is down, and fewer when it's higher. It's more psychological than financial. The fact is that given the market rises, on average, over time, if one has a lump sum to invest, it should be deployed based on other factors, not just DCA'd in. As I said, DCA is just how we all naturally invest from our income.  The above has nothing to do with your situation. You are invested and wish to swap funds. If the funds are with the same broker, you should be able to execute this at the closing price. The sell and buy happen after hours and you wake up the next day with the newly invested portfolio.  If funds are getting transferred from broker to broker, you do have a risk. The risk that they take time, say even 2 days when funds are not invested. A shame to lose a 2% market move as the cost of moving brokers. In this case, I'd do mine and my wife's at different times. To reduce that risk.\",\n          \"1. Increase the use of public transportation options such as public bike sharing, carpooling and public buses.\\n2. Improve public transportation infrastructure, such as adding more stops, introducing night buses and increasing accessibility for disabled passengers.\\n3. Design solutions to reduce overcrowding, such as developing capacity-expanding app-based solutions and introducing dynamic pricing.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}],"source":["df.head()"]},{"cell_type":"code","source":["import json\n","from torch.utils.data import Dataset,DataLoader"],"metadata":{"id":"ahJ2LT67siti","executionInfo":{"status":"ok","timestamp":1737988731080,"user_tz":-60,"elapsed":5,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"EuAXm6wqvO3i","executionInfo":{"status":"ok","timestamp":1737988731081,"user_tz":-60,"elapsed":6,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"708f2058-c2a6-41a9-cde3-2587381c999f"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             instruction  \\\n","1162           When should I walk away from my mortgage?   \n","2279   What makes a Company's Stock prices go up or d...   \n","42511           Describe the key features of a REST API.   \n","40674  Generate a response that conveys an appropriat...   \n","26379          Name three sciences related to computing.   \n","\n","                                                response  \n","1162   This is a very personal situation of course, b...  \n","2279   I always liked the answer that in the short te...  \n","42511  A REST API is a type of API architecture that ...  \n","40674  I apologize for the misunderstanding. Is there...  \n","26379  Three sciences related to computing are comput...  "],"text/html":["\n","  <div id=\"df-7adfa179-050a-4db7-af99-bb43c64c0e24\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instruction</th>\n","      <th>response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1162</th>\n","      <td>When should I walk away from my mortgage?</td>\n","      <td>This is a very personal situation of course, b...</td>\n","    </tr>\n","    <tr>\n","      <th>2279</th>\n","      <td>What makes a Company's Stock prices go up or d...</td>\n","      <td>I always liked the answer that in the short te...</td>\n","    </tr>\n","    <tr>\n","      <th>42511</th>\n","      <td>Describe the key features of a REST API.</td>\n","      <td>A REST API is a type of API architecture that ...</td>\n","    </tr>\n","    <tr>\n","      <th>40674</th>\n","      <td>Generate a response that conveys an appropriat...</td>\n","      <td>I apologize for the misunderstanding. Is there...</td>\n","    </tr>\n","    <tr>\n","      <th>26379</th>\n","      <td>Name three sciences related to computing.</td>\n","      <td>Three sciences related to computing are comput...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7adfa179-050a-4db7-af99-bb43c64c0e24')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7adfa179-050a-4db7-af99-bb43c64c0e24 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7adfa179-050a-4db7-af99-bb43c64c0e24');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4fefaf78-d9fb-4dc3-b4ff-dac4df99d76f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4fefaf78-d9fb-4dc3-b4ff-dac4df99d76f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4fefaf78-d9fb-4dc3-b4ff-dac4df99d76f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 22170,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17688,\n        \"samples\": [\n          \"Create a list of potential guest speakers for a conference\",\n          \"Classify the following observation into a primary level of biological organisation.\",\n          \"Describe a workflow that a customer service representative would use to handle an angry customer.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21956,\n        \"samples\": [\n          \"It is certainly legal to transfer money between people, no matter how often, as long as the money is not originally from illegal sources. If you are gaining in the process, you need to pay taxes on your (net) gain, as on any income; but as always, taxed income is still income. Consider the accumulating transaction cost, the inherent risk (of your friend keeping the money), and the risk of the exchange rate going the other way; but otherwise it is a simple arbitrage business. There are thousands of people who do that all year long at stock exchanges and money markets; you might be able to do it more efficient there, and you don't need a 'friend' on the other side for that.\",\n          \"Dollar cost averaging doesn't (or shouldn't) apply here. DCA is the natural way we invest in the market, buying in by a steady dollar amount each pay period, so over time we can buy more shares when the market is down, and fewer when it's higher. It's more psychological than financial. The fact is that given the market rises, on average, over time, if one has a lump sum to invest, it should be deployed based on other factors, not just DCA'd in. As I said, DCA is just how we all naturally invest from our income.  The above has nothing to do with your situation. You are invested and wish to swap funds. If the funds are with the same broker, you should be able to execute this at the closing price. The sell and buy happen after hours and you wake up the next day with the newly invested portfolio.  If funds are getting transferred from broker to broker, you do have a risk. The risk that they take time, say even 2 days when funds are not invested. A shame to lose a 2% market move as the cost of moving brokers. In this case, I'd do mine and my wife's at different times. To reduce that risk.\",\n          \"1. Increase the use of public transportation options such as public bike sharing, carpooling and public buses.\\n2. Improve public transportation infrastructure, such as adding more stops, introducing night buses and increasing accessibility for disabled passengers.\\n3. Design solutions to reduce overcrowding, such as developing capacity-expanding app-based solutions and introducing dynamic pricing.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def format_input(entry):\n","    instruction_text = f\"### Instruction:\\n{entry['instruction']}\\n\\n### Input:\\n\"\n","    return instruction_text"],"metadata":{"id":"bs1znjG2MmqS","executionInfo":{"status":"ok","timestamp":1737988731544,"user_tz":-60,"elapsed":468,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def collate_fn(\n","    batch,\n","    pad_token_id=50256,\n","    ignore_index=-100,\n","    allowed_max_length=1024,\n","    device = 'cpu'\n","):\n","    # Find the longest sequence in the batch\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs and targets\n","    inputs_lst, targets_lst = [], []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to max_length\n","        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n","        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n","        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n","\n","        # New: Replace all but the first padding tokens in targets by ignore_index\n","        mask = targets == pad_token_id\n","        indices = torch.nonzero(mask).squeeze()\n","        if indices.numel() > 1:\n","            targets[indices[1:]] = ignore_index\n","\n","        # New: Optionally truncate to maximum sequence length\n","        if allowed_max_length is not None:\n","            inputs = inputs[:allowed_max_length]\n","            targets = targets[:allowed_max_length]\n","\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # Convert list of inputs and targets to tensors and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"c5DOPCJoVYpG","executionInfo":{"status":"ok","timestamp":1737988731544,"user_tz":-60,"elapsed":2,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import random\n","#df.to_json(\"final_dataset_clean2.json\")\n","with open(\"final_dataset_clean.json\", \"r\", encoding=\"utf-8\") as file:\n","    data = json.load(file)\n","\n","#data = random.sample(data, k=len(data) // 2)\n","train_size = int(0.8 * len(data))\n","train_df = data[:train_size]\n","valid_df = data[train_size:]\n","\n","class InstructionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","\n","        # Pre-tokenize texts\n","        self.encoded_texts = []\n","        for entry in data:\n","            instruction_plus_input = format_input(entry)\n","            response_text = f\"\\n\\n### Response:\\n{entry['response']}\"\n","            full_text = instruction_plus_input + response_text\n","            self.encoded_texts.append(\n","                tokenizer.encode(full_text)\n","            )\n","\n","    def __getitem__(self, index):\n","        return self.encoded_texts[index]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","CustomDataset = InstructionDataset\n","train_dataset = CustomDataset(train_df, tokenizer)\n","valid_dataset = CustomDataset(valid_df, tokenizer)"],"metadata":{"id":"-mHYI4vsLOUu","executionInfo":{"status":"ok","timestamp":1737988743147,"user_tz":-60,"elapsed":11605,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"vFSxbehSi_iL","executionInfo":{"status":"ok","timestamp":1737988743147,"user_tz":-60,"elapsed":3,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[],"source":["torch.manual_seed(123)\n","from torch.nn.utils.rnn import pad_sequence\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=2,\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0,\n","    collate_fn=lambda batch: collate_fn(batch,device = device)\n",")\n","\n","val_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=2,\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0,\n","    collate_fn=lambda batch: collate_fn(batch,device = device)\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"F_uajP6wi_iN","executionInfo":{"status":"ok","timestamp":1737988744911,"user_tz":-60,"elapsed":1767,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"28f20a8c-02dd-46f2-c876-d0ef5b60d114"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_loss :  3.5319506168365478\n","val_loss :  3.096492338180542\n"]}],"source":["train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","\n","print(\"train_loss : \",train_loss)\n","print(\"val_loss : \", val_loss)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Vir6kzJyi_iO","executionInfo":{"status":"ok","timestamp":1737988750336,"user_tz":-60,"elapsed":5427,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pSa6gWZOi_iO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e86eed80-524f-4569-948b-997533e34815","executionInfo":{"status":"ok","timestamp":1737998444214,"user_tz":-60,"elapsed":9693880,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 3.485, Val loss 3.176\n","Ep 1 (Step 000050): Train loss 2.886, Val loss 2.163\n","Ep 1 (Step 000100): Train loss 2.700, Val loss 2.120\n","Ep 1 (Step 000150): Train loss 2.716, Val loss 2.107\n","Ep 1 (Step 000200): Train loss 2.780, Val loss 2.080\n","Ep 1 (Step 000250): Train loss 2.789, Val loss 2.077\n","Ep 1 (Step 000300): Train loss 2.761, Val loss 2.074\n","Ep 1 (Step 000350): Train loss 2.806, Val loss 2.074\n","Ep 1 (Step 000400): Train loss 2.666, Val loss 2.051\n","Ep 1 (Step 000450): Train loss 2.724, Val loss 2.039\n","Ep 1 (Step 000500): Train loss 2.650, Val loss 2.035\n","Ep 1 (Step 000550): Train loss 2.798, Val loss 2.036\n","Ep 1 (Step 000600): Train loss 2.763, Val loss 2.026\n","Ep 1 (Step 000650): Train loss 2.730, Val loss 2.026\n","Ep 1 (Step 000700): Train loss 2.687, Val loss 2.023\n","Ep 1 (Step 000750): Train loss 2.643, Val loss 2.008\n","Ep 1 (Step 000800): Train loss 2.654, Val loss 2.015\n","Ep 1 (Step 000850): Train loss 2.619, Val loss 2.018\n","Ep 1 (Step 000900): Train loss 2.709, Val loss 2.013\n","Ep 1 (Step 000950): Train loss 2.637, Val loss 2.010\n","Ep 1 (Step 001000): Train loss 2.571, Val loss 2.017\n","Ep 1 (Step 001050): Train loss 2.609, Val loss 2.009\n","Ep 1 (Step 001100): Train loss 2.640, Val loss 2.014\n","Ep 1 (Step 001150): Train loss 2.623, Val loss 2.007\n","Ep 1 (Step 001200): Train loss 2.655, Val loss 2.010\n","Ep 1 (Step 001250): Train loss 2.616, Val loss 2.008\n","Ep 1 (Step 001300): Train loss 2.744, Val loss 2.001\n","Ep 1 (Step 001350): Train loss 2.613, Val loss 2.002\n","Ep 1 (Step 001400): Train loss 2.643, Val loss 2.006\n","Ep 1 (Step 001450): Train loss 2.643, Val loss 2.001\n","Ep 1 (Step 001500): Train loss 2.765, Val loss 2.006\n","Ep 1 (Step 001550): Train loss 2.529, Val loss 2.002\n","Ep 1 (Step 001600): Train loss 2.769, Val loss 1.997\n","Ep 1 (Step 001650): Train loss 2.525, Val loss 1.998\n","Ep 1 (Step 001700): Train loss 2.602, Val loss 1.999\n","Ep 1 (Step 001750): Train loss 2.523, Val loss 1.995\n","Ep 1 (Step 001800): Train loss 2.658, Val loss 1.988\n","Ep 1 (Step 001850): Train loss 2.635, Val loss 1.981\n","Ep 1 (Step 001900): Train loss 2.575, Val loss 1.989\n","Ep 1 (Step 001950): Train loss 2.629, Val loss 1.982\n","Ep 1 (Step 002000): Train loss 2.651, Val loss 1.985\n","Ep 1 (Step 002050): Train loss 2.525, Val loss 1.986\n","Ep 1 (Step 002100): Train loss 2.677, Val loss 1.986\n","Ep 1 (Step 002150): Train loss 2.520, Val loss 1.981\n","Ep 1 (Step 002200): Train loss 2.707, Val loss 1.981\n","Ep 1 (Step 002250): Train loss 2.620, Val loss 1.988\n","Ep 1 (Step 002300): Train loss 2.594, Val loss 1.975\n","Ep 1 (Step 002350): Train loss 2.619, Val loss 1.977\n","Ep 1 (Step 002400): Train loss 2.559, Val loss 1.981\n","Ep 1 (Step 002450): Train loss 2.568, Val loss 1.974\n","Ep 1 (Step 002500): Train loss 2.499, Val loss 1.974\n","Ep 1 (Step 002550): Train loss 2.548, Val loss 1.972\n","Ep 1 (Step 002600): Train loss 2.712, Val loss 1.974\n","Ep 1 (Step 002650): Train loss 2.624, Val loss 1.970\n","Ep 1 (Step 002700): Train loss 2.548, Val loss 1.971\n","Ep 1 (Step 002750): Train loss 2.643, Val loss 1.971\n","Ep 1 (Step 002800): Train loss 2.705, Val loss 1.969\n","Ep 1 (Step 002850): Train loss 2.643, Val loss 1.969\n","Ep 1 (Step 002900): Train loss 2.673, Val loss 1.959\n","Ep 1 (Step 002950): Train loss 2.466, Val loss 1.959\n","Ep 1 (Step 003000): Train loss 2.608, Val loss 1.961\n","Ep 1 (Step 003050): Train loss 2.636, Val loss 1.954\n","Ep 1 (Step 003100): Train loss 2.629, Val loss 1.961\n","Ep 1 (Step 003150): Train loss 2.552, Val loss 1.965\n","Ep 1 (Step 003200): Train loss 2.501, Val loss 1.961\n","Ep 1 (Step 003250): Train loss 2.591, Val loss 1.957\n","Ep 1 (Step 003300): Train loss 2.485, Val loss 1.966\n","Ep 1 (Step 003350): Train loss 2.620, Val loss 1.960\n","Ep 1 (Step 003400): Train loss 2.591, Val loss 1.962\n","Ep 1 (Step 003450): Train loss 2.505, Val loss 1.966\n","Ep 1 (Step 003500): Train loss 2.653, Val loss 1.963\n","Ep 1 (Step 003550): Train loss 2.602, Val loss 1.964\n","Ep 1 (Step 003600): Train loss 2.436, Val loss 1.961\n","Ep 1 (Step 003650): Train loss 2.607, Val loss 1.953\n","Ep 1 (Step 003700): Train loss 2.437, Val loss 1.958\n","Ep 1 (Step 003750): Train loss 2.429, Val loss 1.949\n","Ep 1 (Step 003800): Train loss 2.548, Val loss 1.947\n","Ep 1 (Step 003850): Train loss 2.672, Val loss 1.944\n","Ep 1 (Step 003900): Train loss 2.587, Val loss 1.946\n","Ep 1 (Step 003950): Train loss 2.623, Val loss 1.954\n","Ep 1 (Step 004000): Train loss 2.451, Val loss 1.954\n","Ep 1 (Step 004050): Train loss 2.577, Val loss 1.949\n","Ep 1 (Step 004100): Train loss 2.617, Val loss 1.944\n","Ep 1 (Step 004150): Train loss 2.664, Val loss 1.946\n","Ep 1 (Step 004200): Train loss 2.525, Val loss 1.945\n","Ep 1 (Step 004250): Train loss 2.526, Val loss 1.942\n","Ep 1 (Step 004300): Train loss 2.680, Val loss 1.939\n","Ep 1 (Step 004350): Train loss 2.545, Val loss 1.943\n","Ep 1 (Step 004400): Train loss 2.455, Val loss 1.928\n","Ep 1 (Step 004450): Train loss 2.614, Val loss 1.935\n","Ep 1 (Step 004500): Train loss 2.541, Val loss 1.939\n","Ep 1 (Step 004550): Train loss 2.571, Val loss 1.943\n","Ep 1 (Step 004600): Train loss 2.397, Val loss 1.942\n","Ep 1 (Step 004650): Train loss 2.544, Val loss 1.948\n","Ep 1 (Step 004700): Train loss 2.551, Val loss 1.938\n","Ep 1 (Step 004750): Train loss 2.679, Val loss 1.933\n","Ep 1 (Step 004800): Train loss 2.490, Val loss 1.936\n","Ep 1 (Step 004850): Train loss 2.587, Val loss 1.940\n","Ep 1 (Step 004900): Train loss 2.404, Val loss 1.932\n","Ep 1 (Step 004950): Train loss 2.606, Val loss 1.936\n","Ep 1 (Step 005000): Train loss 2.418, Val loss 1.933\n","Ep 1 (Step 005050): Train loss 2.589, Val loss 1.934\n","Ep 1 (Step 005100): Train loss 2.562, Val loss 1.931\n","Ep 1 (Step 005150): Train loss 2.553, Val loss 1.936\n","Ep 1 (Step 005200): Train loss 2.423, Val loss 1.936\n","Ep 1 (Step 005250): Train loss 2.506, Val loss 1.932\n","Ep 1 (Step 005300): Train loss 2.510, Val loss 1.944\n","Ep 1 (Step 005350): Train loss 2.457, Val loss 1.937\n","Ep 1 (Step 005400): Train loss 2.429, Val loss 1.934\n","Ep 1 (Step 005450): Train loss 2.568, Val loss 1.942\n","Ep 1 (Step 005500): Train loss 2.517, Val loss 1.941\n","Ep 1 (Step 005550): Train loss 2.542, Val loss 1.939\n","Ep 1 (Step 005600): Train loss 2.445, Val loss 1.940\n","Ep 1 (Step 005650): Train loss 2.486, Val loss 1.937\n","Ep 1 (Step 005700): Train loss 2.525, Val loss 1.936\n","Ep 1 (Step 005750): Train loss 2.457, Val loss 1.942\n","Ep 1 (Step 005800): Train loss 2.467, Val loss 1.936\n","Ep 1 (Step 005850): Train loss 2.409, Val loss 1.937\n","Ep 1 (Step 005900): Train loss 2.583, Val loss 1.936\n","Ep 1 (Step 005950): Train loss 2.476, Val loss 1.943\n","Ep 1 (Step 006000): Train loss 2.550, Val loss 1.939\n","Ep 1 (Step 006050): Train loss 2.608, Val loss 1.940\n","Ep 1 (Step 006100): Train loss 2.424, Val loss 1.941\n","Ep 1 (Step 006150): Train loss 2.502, Val loss 1.939\n","Ep 1 (Step 006200): Train loss 2.334, Val loss 1.934\n","Ep 1 (Step 006250): Train loss 2.462, Val loss 1.936\n","Ep 1 (Step 006300): Train loss 2.480, Val loss 1.938\n","Ep 1 (Step 006350): Train loss 2.449, Val loss 1.952\n","Ep 1 (Step 006400): Train loss 2.442, Val loss 1.938\n","Ep 1 (Step 006450): Train loss 2.552, Val loss 1.938\n","Ep 1 (Step 006500): Train loss 2.462, Val loss 1.936\n","Ep 1 (Step 006550): Train loss 2.475, Val loss 1.937\n","Ep 1 (Step 006600): Train loss 2.360, Val loss 1.937\n","Ep 1 (Step 006650): Train loss 2.475, Val loss 1.939\n","Ep 1 (Step 006700): Train loss 2.528, Val loss 1.938\n","Ep 1 (Step 006750): Train loss 2.429, Val loss 1.930\n","Ep 1 (Step 006800): Train loss 2.353, Val loss 1.922\n","Ep 1 (Step 006850): Train loss 2.511, Val loss 1.929\n","Ep 1 (Step 006900): Train loss 2.503, Val loss 1.927\n","Ep 1 (Step 006950): Train loss 2.490, Val loss 1.930\n","Ep 1 (Step 007000): Train loss 2.580, Val loss 1.931\n","Ep 1 (Step 007050): Train loss 2.487, Val loss 1.927\n","Ep 1 (Step 007100): Train loss 2.391, Val loss 1.920\n","Ep 1 (Step 007150): Train loss 2.327, Val loss 1.919\n","Ep 1 (Step 007200): Train loss 2.467, Val loss 1.920\n","Ep 1 (Step 007250): Train loss 2.534, Val loss 1.929\n","Ep 1 (Step 007300): Train loss 2.422, Val loss 1.923\n","Ep 1 (Step 007350): Train loss 2.392, Val loss 1.926\n","Ep 1 (Step 007400): Train loss 2.390, Val loss 1.925\n","Ep 1 (Step 007450): Train loss 2.386, Val loss 1.923\n","Ep 1 (Step 007500): Train loss 2.354, Val loss 1.927\n","Ep 1 (Step 007550): Train loss 2.458, Val loss 1.938\n","Ep 1 (Step 007600): Train loss 2.533, Val loss 1.933\n","Ep 1 (Step 007650): Train loss 2.430, Val loss 1.927\n","Ep 1 (Step 007700): Train loss 2.377, Val loss 1.922\n","Ep 1 (Step 007750): Train loss 2.406, Val loss 1.927\n","Ep 1 (Step 007800): Train loss 2.427, Val loss 1.922\n","Ep 1 (Step 007850): Train loss 2.483, Val loss 1.922\n","Ep 1 (Step 007900): Train loss 2.500, Val loss 1.924\n","Ep 1 (Step 007950): Train loss 2.563, Val loss 1.927\n","Ep 1 (Step 008000): Train loss 2.509, Val loss 1.925\n","Ep 1 (Step 008050): Train loss 2.343, Val loss 1.927\n","Ep 1 (Step 008100): Train loss 2.457, Val loss 1.922\n","Ep 1 (Step 008150): Train loss 2.525, Val loss 1.924\n","Ep 1 (Step 008200): Train loss 2.506, Val loss 1.918\n","Ep 1 (Step 008250): Train loss 2.234, Val loss 1.923\n","Ep 1 (Step 008300): Train loss 2.322, Val loss 1.917\n","Ep 1 (Step 008350): Train loss 2.510, Val loss 1.922\n","Ep 1 (Step 008400): Train loss 2.548, Val loss 1.918\n","Ep 1 (Step 008450): Train loss 2.413, Val loss 1.920\n","Ep 1 (Step 008500): Train loss 2.474, Val loss 1.916\n","Ep 1 (Step 008550): Train loss 2.472, Val loss 1.916\n","Ep 1 (Step 008600): Train loss 2.515, Val loss 1.917\n","Ep 1 (Step 008650): Train loss 2.369, Val loss 1.918\n","Ep 1 (Step 008700): Train loss 2.560, Val loss 1.920\n","Ep 1 (Step 008750): Train loss 2.312, Val loss 1.909\n","Ep 1 (Step 008800): Train loss 2.337, Val loss 1.915\n","Ep 1 (Step 008850): Train loss 2.475, Val loss 1.912\n","Ep 1 (Step 008900): Train loss 2.598, Val loss 1.923\n","Ep 1 (Step 008950): Train loss 2.505, Val loss 1.926\n","Ep 1 (Step 009000): Train loss 2.573, Val loss 1.918\n","Ep 1 (Step 009050): Train loss 2.409, Val loss 1.914\n","Ep 1 (Step 009100): Train loss 2.379, Val loss 1.919\n","Ep 1 (Step 009150): Train loss 2.483, Val loss 1.913\n","Ep 1 (Step 009200): Train loss 2.551, Val loss 1.911\n","Ep 1 (Step 009250): Train loss 2.338, Val loss 1.909\n","Ep 1 (Step 009300): Train loss 2.392, Val loss 1.913\n","Ep 1 (Step 009350): Train loss 2.532, Val loss 1.915\n","Ep 1 (Step 009400): Train loss 2.358, Val loss 1.909\n","Ep 1 (Step 009450): Train loss 2.431, Val loss 1.911\n","Ep 1 (Step 009500): Train loss 2.391, Val loss 1.905\n","Ep 1 (Step 009550): Train loss 2.320, Val loss 1.908\n","Ep 1 (Step 009600): Train loss 2.450, Val loss 1.913\n","Ep 1 (Step 009650): Train loss 2.576, Val loss 1.910\n","Ep 1 (Step 009700): Train loss 2.374, Val loss 1.916\n","Ep 1 (Step 009750): Train loss 2.468, Val loss 1.912\n","Ep 1 (Step 009800): Train loss 2.307, Val loss 1.916\n","Ep 1 (Step 009850): Train loss 2.590, Val loss 1.910\n","Ep 1 (Step 009900): Train loss 2.438, Val loss 1.911\n","Ep 1 (Step 009950): Train loss 2.314, Val loss 1.919\n","Ep 1 (Step 010000): Train loss 2.432, Val loss 1.918\n","Ep 1 (Step 010050): Train loss 2.422, Val loss 1.912\n","Ep 1 (Step 010100): Train loss 2.525, Val loss 1.916\n","Ep 1 (Step 010150): Train loss 2.339, Val loss 1.911\n","Ep 1 (Step 010200): Train loss 2.321, Val loss 1.907\n","Ep 1 (Step 010250): Train loss 2.487, Val loss 1.907\n","Ep 1 (Step 010300): Train loss 2.380, Val loss 1.904\n","Ep 1 (Step 010350): Train loss 2.352, Val loss 1.908\n","Ep 1 (Step 010400): Train loss 2.359, Val loss 1.906\n","Ep 1 (Step 010450): Train loss 2.485, Val loss 1.908\n","Ep 1 (Step 010500): Train loss 2.331, Val loss 1.909\n","Ep 1 (Step 010550): Train loss 2.509, Val loss 1.905\n","Ep 1 (Step 010600): Train loss 2.373, Val loss 1.900\n","Ep 1 (Step 010650): Train loss 2.376, Val loss 1.903\n","Ep 1 (Step 010700): Train loss 2.359, Val loss 1.899\n","Ep 1 (Step 010750): Train loss 2.372, Val loss 1.906\n","Ep 1 (Step 010800): Train loss 2.386, Val loss 1.909\n","Ep 1 (Step 010850): Train loss 2.365, Val loss 1.914\n","Ep 1 (Step 010900): Train loss 2.409, Val loss 1.908\n","Ep 1 (Step 010950): Train loss 2.282, Val loss 1.899\n","Ep 1 (Step 011000): Train loss 2.374, Val loss 1.902\n","Ep 1 (Step 011050): Train loss 2.447, Val loss 1.902\n","Ep 1 (Step 011100): Train loss 2.437, Val loss 1.896\n","Ep 1 (Step 011150): Train loss 2.447, Val loss 1.899\n","Ep 1 (Step 011200): Train loss 2.428, Val loss 1.896\n","Ep 1 (Step 011250): Train loss 2.218, Val loss 1.899\n","Ep 1 (Step 011300): Train loss 2.405, Val loss 1.896\n","Ep 1 (Step 011350): Train loss 2.212, Val loss 1.897\n","Ep 1 (Step 011400): Train loss 2.206, Val loss 1.893\n","Ep 1 (Step 011450): Train loss 2.448, Val loss 1.900\n","Ep 1 (Step 011500): Train loss 2.289, Val loss 1.895\n","Ep 1 (Step 011550): Train loss 2.439, Val loss 1.903\n","Ep 1 (Step 011600): Train loss 2.384, Val loss 1.899\n","Ep 1 (Step 011650): Train loss 2.377, Val loss 1.896\n","Ep 1 (Step 011700): Train loss 2.367, Val loss 1.900\n","Ep 1 (Step 011750): Train loss 2.356, Val loss 1.899\n","Ep 1 (Step 011800): Train loss 2.275, Val loss 1.908\n","Ep 1 (Step 011850): Train loss 2.348, Val loss 1.902\n","Ep 1 (Step 011900): Train loss 2.479, Val loss 1.900\n","Ep 1 (Step 011950): Train loss 2.168, Val loss 1.908\n","Ep 1 (Step 012000): Train loss 2.476, Val loss 1.901\n","Ep 1 (Step 012050): Train loss 2.350, Val loss 1.900\n","Ep 1 (Step 012100): Train loss 2.323, Val loss 1.904\n","Ep 1 (Step 012150): Train loss 2.258, Val loss 1.902\n","Ep 1 (Step 012200): Train loss 2.442, Val loss 1.901\n","Ep 1 (Step 012250): Train loss 2.467, Val loss 1.900\n","Ep 1 (Step 012300): Train loss 2.294, Val loss 1.896\n","Ep 1 (Step 012350): Train loss 2.339, Val loss 1.901\n","Ep 1 (Step 012400): Train loss 2.456, Val loss 1.904\n","Ep 1 (Step 012450): Train loss 2.457, Val loss 1.904\n","Ep 1 (Step 012500): Train loss 2.327, Val loss 1.902\n","Ep 1 (Step 012550): Train loss 2.418, Val loss 1.897\n","Ep 1 (Step 012600): Train loss 2.295, Val loss 1.906\n","Ep 1 (Step 012650): Train loss 2.318, Val loss 1.900\n","Ep 1 (Step 012700): Train loss 2.298, Val loss 1.902\n","Ep 1 (Step 012750): Train loss 2.364, Val loss 1.904\n","Ep 1 (Step 012800): Train loss 2.349, Val loss 1.906\n","Ep 1 (Step 012850): Train loss 2.322, Val loss 1.902\n","Ep 1 (Step 012900): Train loss 2.406, Val loss 1.900\n","Ep 1 (Step 012950): Train loss 2.374, Val loss 1.900\n","Ep 1 (Step 013000): Train loss 2.240, Val loss 1.902\n","Ep 1 (Step 013050): Train loss 2.420, Val loss 1.895\n","Ep 1 (Step 013100): Train loss 2.393, Val loss 1.892\n","Ep 1 (Step 013150): Train loss 2.250, Val loss 1.892\n","Ep 1 (Step 013200): Train loss 2.462, Val loss 1.894\n","Ep 1 (Step 013250): Train loss 2.354, Val loss 1.905\n","Ep 1 (Step 013300): Train loss 2.356, Val loss 1.912\n","Ep 1 (Step 013350): Train loss 2.273, Val loss 1.906\n","Ep 1 (Step 013400): Train loss 2.428, Val loss 1.904\n","Ep 1 (Step 013450): Train loss 2.407, Val loss 1.903\n","Ep 1 (Step 013500): Train loss 2.513, Val loss 1.905\n","Ep 1 (Step 013550): Train loss 2.383, Val loss 1.910\n","Ep 1 (Step 013600): Train loss 2.372, Val loss 1.909\n","Ep 1 (Step 013650): Train loss 2.385, Val loss 1.908\n","Ep 1 (Step 013700): Train loss 2.217, Val loss 1.901\n","Ep 1 (Step 013750): Train loss 2.222, Val loss 1.903\n","Ep 1 (Step 013800): Train loss 2.265, Val loss 1.897\n","Ep 1 (Step 013850): Train loss 2.415, Val loss 1.899\n","Ep 1 (Step 013900): Train loss 2.264, Val loss 1.902\n","Ep 1 (Step 013950): Train loss 2.365, Val loss 1.902\n","Ep 1 (Step 014000): Train loss 2.184, Val loss 1.902\n","Ep 1 (Step 014050): Train loss 2.278, Val loss 1.910\n","Ep 1 (Step 014100): Train loss 2.438, Val loss 1.903\n","Ep 1 (Step 014150): Train loss 2.439, Val loss 1.913\n","Ep 1 (Step 014200): Train loss 2.348, Val loss 1.902\n","Ep 1 (Step 014250): Train loss 2.078, Val loss 1.904\n","Ep 1 (Step 014300): Train loss 2.394, Val loss 1.906\n","Ep 1 (Step 014350): Train loss 2.285, Val loss 1.910\n","Ep 1 (Step 014400): Train loss 2.358, Val loss 1.901\n","Ep 1 (Step 014450): Train loss 2.485, Val loss 1.900\n","Ep 1 (Step 014500): Train loss 2.373, Val loss 1.903\n","Ep 1 (Step 014550): Train loss 2.386, Val loss 1.905\n","Ep 1 (Step 014600): Train loss 2.303, Val loss 1.906\n","Ep 1 (Step 014650): Train loss 2.353, Val loss 1.902\n","Ep 1 (Step 014700): Train loss 2.386, Val loss 1.904\n","Ep 1 (Step 014750): Train loss 2.330, Val loss 1.903\n","Ep 1 (Step 014800): Train loss 2.366, Val loss 1.910\n","Ep 1 (Step 014850): Train loss 2.351, Val loss 1.912\n","Ep 1 (Step 014900): Train loss 2.245, Val loss 1.907\n","Ep 1 (Step 014950): Train loss 2.372, Val loss 1.912\n","Ep 1 (Step 015000): Train loss 2.417, Val loss 1.908\n","Ep 1 (Step 015050): Train loss 2.398, Val loss 1.916\n","Ep 1 (Step 015100): Train loss 2.288, Val loss 1.907\n","Ep 1 (Step 015150): Train loss 2.312, Val loss 1.909\n","Ep 1 (Step 015200): Train loss 2.382, Val loss 1.909\n","Ep 1 (Step 015250): Train loss 2.352, Val loss 1.912\n","Ep 1 (Step 015300): Train loss 2.273, Val loss 1.910\n","Ep 1 (Step 015350): Train loss 2.351, Val loss 1.918\n","Ep 1 (Step 015400): Train loss 2.359, Val loss 1.915\n","Ep 1 (Step 015450): Train loss 2.378, Val loss 1.912\n","Ep 1 (Step 015500): Train loss 2.264, Val loss 1.914\n","Ep 1 (Step 015550): Train loss 2.352, Val loss 1.909\n","Ep 1 (Step 015600): Train loss 2.304, Val loss 1.906\n","Ep 1 (Step 015650): Train loss 2.265, Val loss 1.910\n","Ep 1 (Step 015700): Train loss 2.398, Val loss 1.909\n","Ep 1 (Step 015750): Train loss 2.430, Val loss 1.905\n","Ep 1 (Step 015800): Train loss 2.254, Val loss 1.904\n","Ep 1 (Step 015850): Train loss 2.316, Val loss 1.916\n","Ep 1 (Step 015900): Train loss 2.298, Val loss 1.908\n","Ep 1 (Step 015950): Train loss 2.343, Val loss 1.904\n","Ep 1 (Step 016000): Train loss 2.568, Val loss 1.909\n","Ep 1 (Step 016050): Train loss 2.232, Val loss 1.905\n","Ep 1 (Step 016100): Train loss 2.302, Val loss 1.906\n","Ep 1 (Step 016150): Train loss 2.255, Val loss 1.905\n","Ep 1 (Step 016200): Train loss 2.229, Val loss 1.899\n","Ep 1 (Step 016250): Train loss 2.337, Val loss 1.906\n","Ep 1 (Step 016300): Train loss 2.264, Val loss 1.908\n","Ep 1 (Step 016350): Train loss 2.322, Val loss 1.905\n","Ep 1 (Step 016400): Train loss 2.262, Val loss 1.901\n","Ep 1 (Step 016450): Train loss 2.344, Val loss 1.902\n","Ep 1 (Step 016500): Train loss 2.258, Val loss 1.904\n","Ep 1 (Step 016550): Train loss 2.361, Val loss 1.903\n","Ep 1 (Step 016600): Train loss 2.238, Val loss 1.899\n","Ep 1 (Step 016650): Train loss 2.290, Val loss 1.899\n","Ep 1 (Step 016700): Train loss 2.369, Val loss 1.898\n","Ep 1 (Step 016750): Train loss 2.376, Val loss 1.898\n","Ep 1 (Step 016800): Train loss 2.290, Val loss 1.900\n","Ep 1 (Step 016850): Train loss 2.362, Val loss 1.903\n","Ep 1 (Step 016900): Train loss 2.327, Val loss 1.898\n","Ep 1 (Step 016950): Train loss 2.284, Val loss 1.895\n","Ep 1 (Step 017000): Train loss 2.210, Val loss 1.899\n","Ep 1 (Step 017050): Train loss 2.269, Val loss 1.900\n","Ep 1 (Step 017100): Train loss 2.406, Val loss 1.893\n","Ep 1 (Step 017150): Train loss 2.340, Val loss 1.896\n","Ep 1 (Step 017200): Train loss 2.345, Val loss 1.895\n","Ep 1 (Step 017250): Train loss 2.349, Val loss 1.896\n","Ep 1 (Step 017300): Train loss 2.175, Val loss 1.894\n","Ep 1 (Step 017350): Train loss 2.182, Val loss 1.898\n","Ep 1 (Step 017400): Train loss 2.180, Val loss 1.899\n","Ep 1 (Step 017450): Train loss 2.338, Val loss 1.900\n","Ep 1 (Step 017500): Train loss 2.329, Val loss 1.899\n","Ep 1 (Step 017550): Train loss 2.170, Val loss 1.895\n","Ep 1 (Step 017600): Train loss 2.225, Val loss 1.903\n","Ep 1 (Step 017650): Train loss 2.388, Val loss 1.903\n","Ep 1 (Step 017700): Train loss 2.386, Val loss 1.902\n","### Instruction: Detect hate speech in the following sentence:  ### Input: The sentence contains hate speech.<|endoftext|>The following is a list of the most common types of hate speech.  The most common types of hate speech are:  - Hatred - Hatred - Hatred - Hatred \n","Ep 2 (Step 017750): Train loss 2.168, Val loss 1.900\n","Ep 2 (Step 017800): Train loss 2.321, Val loss 1.918\n","Ep 2 (Step 017850): Train loss 2.316, Val loss 1.917\n","Ep 2 (Step 017900): Train loss 2.248, Val loss 1.925\n","Ep 2 (Step 017950): Train loss 2.239, Val loss 1.927\n","Ep 2 (Step 018000): Train loss 2.249, Val loss 1.911\n","Ep 2 (Step 018050): Train loss 2.225, Val loss 1.913\n","Ep 2 (Step 018100): Train loss 2.208, Val loss 1.919\n","Ep 2 (Step 018150): Train loss 2.272, Val loss 1.918\n","Ep 2 (Step 018200): Train loss 2.313, Val loss 1.908\n","Ep 2 (Step 018250): Train loss 2.343, Val loss 1.906\n","Ep 2 (Step 018300): Train loss 2.366, Val loss 1.906\n","Ep 2 (Step 018350): Train loss 2.305, Val loss 1.910\n","Ep 2 (Step 018400): Train loss 2.387, Val loss 1.914\n","Ep 2 (Step 018450): Train loss 2.380, Val loss 1.913\n","Ep 2 (Step 018500): Train loss 2.263, Val loss 1.914\n","Ep 2 (Step 018550): Train loss 2.310, Val loss 1.916\n","Ep 2 (Step 018600): Train loss 2.175, Val loss 1.914\n","Ep 2 (Step 018650): Train loss 2.268, Val loss 1.913\n","Ep 2 (Step 018700): Train loss 2.322, Val loss 1.909\n","Ep 2 (Step 018750): Train loss 2.158, Val loss 1.906\n","Ep 2 (Step 018800): Train loss 2.278, Val loss 1.900\n","Ep 2 (Step 018850): Train loss 2.269, Val loss 1.905\n","Ep 2 (Step 018900): Train loss 2.262, Val loss 1.897\n","Ep 2 (Step 018950): Train loss 2.243, Val loss 1.904\n","Ep 2 (Step 019000): Train loss 2.384, Val loss 1.926\n","Ep 2 (Step 019050): Train loss 2.169, Val loss 1.915\n","Ep 2 (Step 019100): Train loss 2.204, Val loss 1.910\n","Ep 2 (Step 019150): Train loss 2.105, Val loss 1.918\n","Ep 2 (Step 019200): Train loss 2.127, Val loss 1.925\n","Ep 2 (Step 019250): Train loss 2.284, Val loss 1.913\n","Ep 2 (Step 019300): Train loss 2.245, Val loss 1.913\n","Ep 2 (Step 019350): Train loss 2.277, Val loss 1.921\n","Ep 2 (Step 019400): Train loss 2.383, Val loss 1.910\n","Ep 2 (Step 019450): Train loss 2.218, Val loss 1.905\n","Ep 2 (Step 019500): Train loss 2.167, Val loss 1.900\n","Ep 2 (Step 019550): Train loss 2.279, Val loss 1.906\n","Ep 2 (Step 019600): Train loss 2.239, Val loss 1.912\n","Ep 2 (Step 019650): Train loss 2.185, Val loss 1.915\n","Ep 2 (Step 019700): Train loss 2.335, Val loss 1.918\n","Ep 2 (Step 019750): Train loss 2.333, Val loss 1.917\n","Ep 2 (Step 019800): Train loss 2.169, Val loss 1.909\n","Ep 2 (Step 019850): Train loss 2.179, Val loss 1.917\n","Ep 2 (Step 019900): Train loss 2.411, Val loss 1.914\n","Ep 2 (Step 019950): Train loss 2.180, Val loss 1.915\n","Ep 2 (Step 020000): Train loss 2.310, Val loss 1.920\n","Ep 2 (Step 020050): Train loss 2.262, Val loss 1.907\n","Ep 2 (Step 020100): Train loss 2.214, Val loss 1.919\n","Ep 2 (Step 020150): Train loss 2.360, Val loss 1.911\n","Ep 2 (Step 020200): Train loss 2.112, Val loss 1.916\n","Ep 2 (Step 020250): Train loss 2.225, Val loss 1.925\n","Ep 2 (Step 020300): Train loss 2.184, Val loss 1.927\n","Ep 2 (Step 020350): Train loss 2.363, Val loss 1.927\n","Ep 2 (Step 020400): Train loss 2.397, Val loss 1.927\n","Ep 2 (Step 020450): Train loss 2.262, Val loss 1.919\n","Ep 2 (Step 020500): Train loss 2.270, Val loss 1.921\n","Ep 2 (Step 020550): Train loss 2.223, Val loss 1.917\n","Ep 2 (Step 020600): Train loss 2.250, Val loss 1.922\n","Ep 2 (Step 020650): Train loss 2.261, Val loss 1.918\n","Ep 2 (Step 020700): Train loss 2.189, Val loss 1.907\n","Ep 2 (Step 020750): Train loss 2.275, Val loss 1.917\n","Ep 2 (Step 020800): Train loss 2.109, Val loss 1.926\n","Ep 2 (Step 020850): Train loss 2.247, Val loss 1.919\n","Ep 2 (Step 020900): Train loss 2.296, Val loss 1.920\n","Ep 2 (Step 020950): Train loss 2.222, Val loss 1.921\n","Ep 2 (Step 021000): Train loss 2.383, Val loss 1.915\n","Ep 2 (Step 021050): Train loss 2.312, Val loss 1.905\n","Ep 2 (Step 021100): Train loss 2.275, Val loss 1.905\n","Ep 2 (Step 021150): Train loss 2.205, Val loss 1.914\n","Ep 2 (Step 021200): Train loss 2.225, Val loss 1.913\n","Ep 2 (Step 021250): Train loss 2.163, Val loss 1.918\n","Ep 2 (Step 021300): Train loss 2.309, Val loss 1.910\n","Ep 2 (Step 021350): Train loss 2.263, Val loss 1.917\n","Ep 2 (Step 021400): Train loss 2.257, Val loss 1.909\n","Ep 2 (Step 021450): Train loss 2.151, Val loss 1.915\n","Ep 2 (Step 021500): Train loss 2.267, Val loss 1.919\n","Ep 2 (Step 021550): Train loss 2.170, Val loss 1.917\n","Ep 2 (Step 021600): Train loss 2.271, Val loss 1.920\n","Ep 2 (Step 021650): Train loss 2.253, Val loss 1.913\n","Ep 2 (Step 021700): Train loss 2.288, Val loss 1.924\n","Ep 2 (Step 021750): Train loss 2.230, Val loss 1.921\n","Ep 2 (Step 021800): Train loss 2.233, Val loss 1.917\n","Ep 2 (Step 021850): Train loss 2.129, Val loss 1.919\n","Ep 2 (Step 021900): Train loss 2.172, Val loss 1.927\n","Ep 2 (Step 021950): Train loss 2.253, Val loss 1.918\n","Ep 2 (Step 022000): Train loss 2.296, Val loss 1.918\n","Ep 2 (Step 022050): Train loss 2.163, Val loss 1.923\n","Ep 2 (Step 022100): Train loss 2.290, Val loss 1.916\n","Ep 2 (Step 022150): Train loss 2.240, Val loss 1.914\n","Ep 2 (Step 022200): Train loss 2.253, Val loss 1.912\n","Ep 2 (Step 022250): Train loss 2.003, Val loss 1.915\n","Ep 2 (Step 022300): Train loss 2.214, Val loss 1.917\n","Ep 2 (Step 022350): Train loss 2.301, Val loss 1.917\n","Ep 2 (Step 022400): Train loss 2.086, Val loss 1.917\n","Ep 2 (Step 022450): Train loss 2.290, Val loss 1.919\n","Ep 2 (Step 022500): Train loss 2.201, Val loss 1.916\n","Ep 2 (Step 022550): Train loss 2.206, Val loss 1.919\n","Ep 2 (Step 022600): Train loss 2.344, Val loss 1.928\n","Ep 2 (Step 022650): Train loss 2.232, Val loss 1.923\n","Ep 2 (Step 022700): Train loss 2.238, Val loss 1.917\n","Ep 2 (Step 022750): Train loss 2.000, Val loss 1.922\n","Ep 2 (Step 022800): Train loss 2.244, Val loss 1.916\n","Ep 2 (Step 022850): Train loss 2.208, Val loss 1.913\n","Ep 2 (Step 022900): Train loss 2.237, Val loss 1.925\n","Ep 2 (Step 022950): Train loss 2.218, Val loss 1.929\n","Ep 2 (Step 023000): Train loss 2.205, Val loss 1.923\n","Ep 2 (Step 023050): Train loss 2.217, Val loss 1.925\n","Ep 2 (Step 023100): Train loss 2.255, Val loss 1.933\n","Ep 2 (Step 023150): Train loss 2.264, Val loss 1.932\n","Ep 2 (Step 023200): Train loss 2.187, Val loss 1.931\n","Ep 2 (Step 023250): Train loss 2.247, Val loss 1.942\n","Ep 2 (Step 023300): Train loss 2.244, Val loss 1.929\n","Ep 2 (Step 023350): Train loss 2.174, Val loss 1.933\n","Ep 2 (Step 023400): Train loss 2.261, Val loss 1.934\n","Ep 2 (Step 023450): Train loss 2.267, Val loss 1.936\n","Ep 2 (Step 023500): Train loss 2.223, Val loss 1.923\n","Ep 2 (Step 023550): Train loss 2.229, Val loss 1.921\n","Ep 2 (Step 023600): Train loss 2.238, Val loss 1.927\n","Ep 2 (Step 023650): Train loss 2.224, Val loss 1.919\n","Ep 2 (Step 023700): Train loss 2.188, Val loss 1.920\n","Ep 2 (Step 023750): Train loss 2.241, Val loss 1.920\n","Ep 2 (Step 023800): Train loss 2.061, Val loss 1.926\n","Ep 2 (Step 023850): Train loss 2.186, Val loss 1.932\n","Ep 2 (Step 023900): Train loss 2.161, Val loss 1.923\n","Ep 2 (Step 023950): Train loss 2.229, Val loss 1.925\n","Ep 2 (Step 024000): Train loss 2.130, Val loss 1.934\n","Ep 2 (Step 024050): Train loss 2.114, Val loss 1.930\n","Ep 2 (Step 024100): Train loss 2.070, Val loss 1.926\n","Ep 2 (Step 024150): Train loss 2.168, Val loss 1.926\n","Ep 2 (Step 024200): Train loss 2.056, Val loss 1.929\n","Ep 2 (Step 024250): Train loss 2.162, Val loss 1.924\n","Ep 2 (Step 024300): Train loss 2.160, Val loss 1.921\n","Ep 2 (Step 024350): Train loss 2.249, Val loss 1.926\n","Ep 2 (Step 024400): Train loss 2.340, Val loss 1.926\n","Ep 2 (Step 024450): Train loss 2.213, Val loss 1.917\n","Ep 2 (Step 024500): Train loss 2.068, Val loss 1.924\n","Ep 2 (Step 024550): Train loss 2.166, Val loss 1.924\n","Ep 2 (Step 024600): Train loss 2.237, Val loss 1.922\n","Ep 2 (Step 024650): Train loss 2.348, Val loss 1.920\n","Ep 2 (Step 024700): Train loss 2.197, Val loss 1.925\n","Ep 2 (Step 024750): Train loss 2.345, Val loss 1.924\n","Ep 2 (Step 024800): Train loss 2.155, Val loss 1.926\n","Ep 2 (Step 024850): Train loss 2.354, Val loss 1.923\n","Ep 2 (Step 024900): Train loss 2.159, Val loss 1.920\n","Ep 2 (Step 024950): Train loss 2.088, Val loss 1.920\n","Ep 2 (Step 025000): Train loss 2.298, Val loss 1.919\n","Ep 2 (Step 025050): Train loss 2.105, Val loss 1.920\n","Ep 2 (Step 025100): Train loss 2.151, Val loss 1.912\n","Ep 2 (Step 025150): Train loss 2.265, Val loss 1.918\n","Ep 2 (Step 025200): Train loss 2.167, Val loss 1.932\n","Ep 2 (Step 025250): Train loss 2.198, Val loss 1.929\n","Ep 2 (Step 025300): Train loss 2.257, Val loss 1.926\n","Ep 2 (Step 025350): Train loss 2.283, Val loss 1.925\n","Ep 2 (Step 025400): Train loss 2.172, Val loss 1.923\n","Ep 2 (Step 025450): Train loss 2.136, Val loss 1.922\n","Ep 2 (Step 025500): Train loss 2.209, Val loss 1.923\n","Ep 2 (Step 025550): Train loss 2.257, Val loss 1.929\n","Ep 2 (Step 025600): Train loss 2.222, Val loss 1.934\n","Ep 2 (Step 025650): Train loss 2.261, Val loss 1.931\n","Ep 2 (Step 025700): Train loss 2.240, Val loss 1.919\n","Ep 2 (Step 025750): Train loss 2.079, Val loss 1.924\n","Ep 2 (Step 025800): Train loss 2.041, Val loss 1.924\n","Ep 2 (Step 025850): Train loss 2.084, Val loss 1.917\n","Ep 2 (Step 025900): Train loss 2.149, Val loss 1.918\n","Ep 2 (Step 025950): Train loss 2.159, Val loss 1.920\n","Ep 2 (Step 026000): Train loss 2.022, Val loss 1.922\n","Ep 2 (Step 026050): Train loss 2.291, Val loss 1.918\n","Ep 2 (Step 026100): Train loss 2.091, Val loss 1.915\n","Ep 2 (Step 026150): Train loss 2.235, Val loss 1.924\n","Ep 2 (Step 026200): Train loss 2.141, Val loss 1.907\n","Ep 2 (Step 026250): Train loss 2.055, Val loss 1.920\n","Ep 2 (Step 026300): Train loss 2.091, Val loss 1.912\n","Ep 2 (Step 026350): Train loss 2.069, Val loss 1.917\n","Ep 2 (Step 026400): Train loss 2.085, Val loss 1.918\n","Ep 2 (Step 026450): Train loss 2.108, Val loss 1.929\n","Ep 2 (Step 026500): Train loss 2.149, Val loss 1.921\n","Ep 2 (Step 026550): Train loss 2.119, Val loss 1.920\n","Ep 2 (Step 026600): Train loss 2.195, Val loss 1.926\n","Ep 2 (Step 026650): Train loss 2.184, Val loss 1.930\n","Ep 2 (Step 026700): Train loss 2.137, Val loss 1.927\n","Ep 2 (Step 026750): Train loss 2.157, Val loss 1.925\n","Ep 2 (Step 026800): Train loss 2.103, Val loss 1.916\n","Ep 2 (Step 026850): Train loss 2.037, Val loss 1.924\n","Ep 2 (Step 026900): Train loss 1.980, Val loss 1.921\n","Ep 2 (Step 026950): Train loss 2.350, Val loss 1.927\n","Ep 2 (Step 027000): Train loss 2.175, Val loss 1.926\n","Ep 2 (Step 027050): Train loss 2.063, Val loss 1.932\n","Ep 2 (Step 027100): Train loss 2.092, Val loss 1.934\n","Ep 2 (Step 027150): Train loss 2.207, Val loss 1.916\n","Ep 2 (Step 027200): Train loss 2.311, Val loss 1.925\n","Ep 2 (Step 027250): Train loss 2.134, Val loss 1.932\n","Ep 2 (Step 027300): Train loss 2.026, Val loss 1.930\n","Ep 2 (Step 027350): Train loss 2.166, Val loss 1.919\n","Ep 2 (Step 027400): Train loss 2.089, Val loss 1.941\n","Ep 2 (Step 027450): Train loss 2.062, Val loss 1.924\n","Ep 2 (Step 027500): Train loss 2.119, Val loss 1.917\n","Ep 2 (Step 027550): Train loss 2.116, Val loss 1.918\n","Ep 2 (Step 027600): Train loss 1.920, Val loss 1.919\n","Ep 2 (Step 027650): Train loss 2.221, Val loss 1.913\n","Ep 2 (Step 027700): Train loss 2.161, Val loss 1.921\n","Ep 2 (Step 027750): Train loss 2.189, Val loss 1.920\n","Ep 2 (Step 027800): Train loss 2.247, Val loss 1.917\n","Ep 2 (Step 027850): Train loss 2.200, Val loss 1.921\n","Ep 2 (Step 027900): Train loss 2.105, Val loss 1.931\n","Ep 2 (Step 027950): Train loss 2.100, Val loss 1.921\n","Ep 2 (Step 028000): Train loss 2.168, Val loss 1.917\n","Ep 2 (Step 028050): Train loss 2.060, Val loss 1.920\n","Ep 2 (Step 028100): Train loss 2.168, Val loss 1.921\n","Ep 2 (Step 028150): Train loss 2.188, Val loss 1.918\n","Ep 2 (Step 028200): Train loss 2.125, Val loss 1.918\n","Ep 2 (Step 028250): Train loss 2.068, Val loss 1.928\n","Ep 2 (Step 028300): Train loss 2.276, Val loss 1.921\n","Ep 2 (Step 028350): Train loss 2.047, Val loss 1.916\n","Ep 2 (Step 028400): Train loss 2.174, Val loss 1.917\n","Ep 2 (Step 028450): Train loss 2.273, Val loss 1.920\n","Ep 2 (Step 028500): Train loss 2.365, Val loss 1.918\n","Ep 2 (Step 028550): Train loss 2.196, Val loss 1.925\n","Ep 2 (Step 028600): Train loss 2.134, Val loss 1.925\n","Ep 2 (Step 028650): Train loss 2.110, Val loss 1.928\n","Ep 2 (Step 028700): Train loss 2.381, Val loss 1.929\n","Ep 2 (Step 028750): Train loss 2.151, Val loss 1.924\n","Ep 2 (Step 028800): Train loss 2.144, Val loss 1.931\n","Ep 2 (Step 028850): Train loss 2.188, Val loss 1.924\n","Ep 2 (Step 028900): Train loss 2.244, Val loss 1.941\n","Ep 2 (Step 028950): Train loss 2.145, Val loss 1.932\n","Ep 2 (Step 029000): Train loss 2.273, Val loss 1.934\n","Ep 2 (Step 029050): Train loss 2.074, Val loss 1.932\n","Ep 2 (Step 029100): Train loss 2.168, Val loss 1.939\n","Ep 2 (Step 029150): Train loss 2.156, Val loss 1.926\n","Ep 2 (Step 029200): Train loss 2.269, Val loss 1.935\n","Ep 2 (Step 029250): Train loss 2.213, Val loss 1.928\n","Ep 2 (Step 029300): Train loss 2.207, Val loss 1.931\n","Ep 2 (Step 029350): Train loss 2.226, Val loss 1.938\n","Ep 2 (Step 029400): Train loss 2.159, Val loss 1.936\n","Ep 2 (Step 029450): Train loss 2.261, Val loss 1.917\n","Ep 2 (Step 029500): Train loss 2.041, Val loss 1.926\n","Ep 2 (Step 029550): Train loss 2.019, Val loss 1.929\n","Ep 2 (Step 029600): Train loss 2.277, Val loss 1.931\n","Ep 2 (Step 029650): Train loss 2.244, Val loss 1.928\n","Ep 2 (Step 029700): Train loss 2.239, Val loss 1.922\n","Ep 2 (Step 029750): Train loss 2.219, Val loss 1.931\n","Ep 2 (Step 029800): Train loss 2.176, Val loss 1.927\n","Ep 2 (Step 029850): Train loss 2.098, Val loss 1.924\n","Ep 2 (Step 029900): Train loss 2.075, Val loss 1.931\n","Ep 2 (Step 029950): Train loss 2.282, Val loss 1.930\n","Ep 2 (Step 030000): Train loss 2.142, Val loss 1.926\n","Ep 2 (Step 030050): Train loss 2.223, Val loss 1.935\n","Ep 2 (Step 030100): Train loss 2.178, Val loss 1.922\n","Ep 2 (Step 030150): Train loss 2.169, Val loss 1.929\n","Ep 2 (Step 030200): Train loss 2.363, Val loss 1.924\n","Ep 2 (Step 030250): Train loss 2.057, Val loss 1.919\n","Ep 2 (Step 030300): Train loss 2.173, Val loss 1.915\n","Ep 2 (Step 030350): Train loss 2.163, Val loss 1.926\n","Ep 2 (Step 030400): Train loss 2.199, Val loss 1.926\n","Ep 2 (Step 030450): Train loss 2.112, Val loss 1.924\n","Ep 2 (Step 030500): Train loss 2.160, Val loss 1.929\n","Ep 2 (Step 030550): Train loss 2.119, Val loss 1.929\n","Ep 2 (Step 030600): Train loss 2.229, Val loss 1.934\n","Ep 2 (Step 030650): Train loss 2.017, Val loss 1.934\n","Ep 2 (Step 030700): Train loss 2.049, Val loss 1.937\n","Ep 2 (Step 030750): Train loss 2.044, Val loss 1.935\n","Ep 2 (Step 030800): Train loss 2.083, Val loss 1.930\n","Ep 2 (Step 030850): Train loss 2.264, Val loss 1.939\n","Ep 2 (Step 030900): Train loss 2.059, Val loss 1.938\n","Ep 2 (Step 030950): Train loss 2.249, Val loss 1.937\n","Ep 2 (Step 031000): Train loss 2.124, Val loss 1.936\n","Ep 2 (Step 031050): Train loss 2.133, Val loss 1.933\n","Ep 2 (Step 031100): Train loss 2.036, Val loss 1.931\n","Ep 2 (Step 031150): Train loss 1.922, Val loss 1.925\n","Ep 2 (Step 031200): Train loss 2.130, Val loss 1.924\n","Ep 2 (Step 031250): Train loss 2.083, Val loss 1.917\n","Ep 2 (Step 031300): Train loss 2.153, Val loss 1.918\n","Ep 2 (Step 031350): Train loss 2.130, Val loss 1.920\n","Ep 2 (Step 031400): Train loss 2.161, Val loss 1.930\n","Ep 2 (Step 031450): Train loss 2.049, Val loss 1.932\n","Ep 2 (Step 031500): Train loss 2.134, Val loss 1.921\n","Ep 2 (Step 031550): Train loss 2.123, Val loss 1.920\n","Ep 2 (Step 031600): Train loss 2.004, Val loss 1.924\n","Ep 2 (Step 031650): Train loss 2.097, Val loss 1.914\n","Ep 2 (Step 031700): Train loss 2.169, Val loss 1.915\n","Ep 2 (Step 031750): Train loss 1.969, Val loss 1.931\n","Ep 2 (Step 031800): Train loss 2.109, Val loss 1.909\n","Ep 2 (Step 031850): Train loss 2.179, Val loss 1.917\n","Ep 2 (Step 031900): Train loss 2.099, Val loss 1.915\n","Ep 2 (Step 031950): Train loss 2.192, Val loss 1.927\n","Ep 2 (Step 032000): Train loss 2.065, Val loss 1.923\n","Ep 2 (Step 032050): Train loss 2.053, Val loss 1.921\n","Ep 2 (Step 032100): Train loss 2.078, Val loss 1.923\n","Ep 2 (Step 032150): Train loss 2.099, Val loss 1.932\n","Ep 2 (Step 032200): Train loss 2.058, Val loss 1.928\n","Ep 2 (Step 032250): Train loss 2.131, Val loss 1.917\n","Ep 2 (Step 032300): Train loss 2.063, Val loss 1.913\n","Ep 2 (Step 032350): Train loss 2.056, Val loss 1.919\n","Ep 2 (Step 032400): Train loss 2.226, Val loss 1.921\n","Ep 2 (Step 032450): Train loss 2.067, Val loss 1.922\n","Ep 2 (Step 032500): Train loss 2.146, Val loss 1.917\n","Ep 2 (Step 032550): Train loss 2.163, Val loss 1.924\n","Ep 2 (Step 032600): Train loss 2.206, Val loss 1.916\n","Ep 2 (Step 032650): Train loss 2.035, Val loss 1.916\n","Ep 2 (Step 032700): Train loss 2.039, Val loss 1.917\n","Ep 2 (Step 032750): Train loss 2.028, Val loss 1.913\n","Ep 2 (Step 032800): Train loss 2.209, Val loss 1.916\n","Ep 2 (Step 032850): Train loss 2.062, Val loss 1.916\n","Ep 2 (Step 032900): Train loss 1.989, Val loss 1.913\n","Ep 2 (Step 032950): Train loss 2.087, Val loss 1.918\n","Ep 2 (Step 033000): Train loss 2.148, Val loss 1.922\n","Ep 2 (Step 033050): Train loss 2.116, Val loss 1.921\n","Ep 2 (Step 033100): Train loss 2.083, Val loss 1.922\n","Ep 2 (Step 033150): Train loss 2.219, Val loss 1.923\n","Ep 2 (Step 033200): Train loss 2.006, Val loss 1.924\n","Ep 2 (Step 033250): Train loss 2.186, Val loss 1.926\n","Ep 2 (Step 033300): Train loss 2.153, Val loss 1.927\n","Ep 2 (Step 033350): Train loss 2.023, Val loss 1.912\n","Ep 2 (Step 033400): Train loss 2.044, Val loss 1.916\n","Ep 2 (Step 033450): Train loss 2.143, Val loss 1.908\n","Ep 2 (Step 033500): Train loss 2.120, Val loss 1.913\n","Ep 2 (Step 033550): Train loss 1.958, Val loss 1.923\n","Ep 2 (Step 033600): Train loss 2.209, Val loss 1.925\n","Ep 2 (Step 033650): Train loss 2.041, Val loss 1.922\n","Ep 2 (Step 033700): Train loss 2.082, Val loss 1.917\n","Ep 2 (Step 033750): Train loss 2.127, Val loss 1.917\n","Ep 2 (Step 033800): Train loss 2.161, Val loss 1.919\n","Ep 2 (Step 033850): Train loss 2.143, Val loss 1.924\n","Ep 2 (Step 033900): Train loss 2.226, Val loss 1.913\n","Ep 2 (Step 033950): Train loss 2.132, Val loss 1.915\n","Ep 2 (Step 034000): Train loss 1.990, Val loss 1.910\n","Ep 2 (Step 034050): Train loss 2.113, Val loss 1.918\n","Ep 2 (Step 034100): Train loss 2.079, Val loss 1.920\n","Ep 2 (Step 034150): Train loss 2.129, Val loss 1.916\n","Ep 2 (Step 034200): Train loss 2.169, Val loss 1.924\n","Ep 2 (Step 034250): Train loss 2.081, Val loss 1.926\n","Ep 2 (Step 034300): Train loss 1.970, Val loss 1.926\n","Ep 2 (Step 034350): Train loss 1.996, Val loss 1.922\n","Ep 2 (Step 034400): Train loss 2.240, Val loss 1.924\n","Ep 2 (Step 034450): Train loss 1.984, Val loss 1.920\n","Ep 2 (Step 034500): Train loss 2.102, Val loss 1.918\n","Ep 2 (Step 034550): Train loss 2.074, Val loss 1.917\n","Ep 2 (Step 034600): Train loss 2.039, Val loss 1.920\n","Ep 2 (Step 034650): Train loss 2.032, Val loss 1.920\n","Ep 2 (Step 034700): Train loss 2.078, Val loss 1.925\n","Ep 2 (Step 034750): Train loss 2.241, Val loss 1.927\n","Ep 2 (Step 034800): Train loss 2.169, Val loss 1.921\n","Ep 2 (Step 034850): Train loss 1.991, Val loss 1.918\n","Ep 2 (Step 034900): Train loss 2.007, Val loss 1.919\n","Ep 2 (Step 034950): Train loss 2.048, Val loss 1.914\n","Ep 2 (Step 035000): Train loss 2.212, Val loss 1.925\n","Ep 2 (Step 035050): Train loss 2.073, Val loss 1.921\n","Ep 2 (Step 035100): Train loss 1.955, Val loss 1.918\n","Ep 2 (Step 035150): Train loss 2.075, Val loss 1.921\n","Ep 2 (Step 035200): Train loss 2.000, Val loss 1.911\n","Ep 2 (Step 035250): Train loss 2.215, Val loss 1.924\n","Ep 2 (Step 035300): Train loss 2.020, Val loss 1.926\n","Ep 2 (Step 035350): Train loss 1.980, Val loss 1.926\n","Ep 2 (Step 035400): Train loss 2.208, Val loss 1.920\n","Ep 2 (Step 035450): Train loss 2.141, Val loss 1.920\n","### Instruction: Detect hate speech in the following sentence:  ### Input: \"I am a Muslim\"<|endoftext|>The following sentence contains a simile. It is a metaphor used to describe a person who is feeling sad or lonely.<|endoftext|>The following sentence contains a simile. It is a metaphor used to describe a person who\n"]}],"source":["train_losses, val_losses, tokens_seen = train_model_simple(\n","        model, train_loader, val_loader, optimizer, device,\n","        num_epochs=2, eval_freq=50, eval_iter=50,\n","        start_context=format_input(valid_df[0]), tokenizer=tokenizer\n","    )"]},{"cell_type":"code","source":["print(f\"Taille du dataset d'entraînement : {len(train_dataset)}\")\n","print(f\"Batch size utilisé : {train_loader.batch_size}\")\n"],"metadata":{"id":"mc-CILcWdSZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737998444215,"user_tz":-60,"elapsed":22,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"outputId":"2e83cf09-952e-4dbe-816e-974a95057c1a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Taille du dataset d'entraînement : 35472\n","Batch size utilisé : 2\n"]}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"5IrboDjZi_iP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737998444592,"user_tz":-60,"elapsed":383,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}},"outputId":"41284643-0842-48db-bd39-5e2684f5537f"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_loss after finetuning :  2.4190662145614623\n","val_loss after finetuning :  1.8173839092254638\n"]}],"source":["train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","\n","print(\"train_loss after finetuning : \",train_loss)\n","print(\"val_loss after finetuning : \", val_loss)"]},{"cell_type":"code","source":[],"metadata":{"id":"bxx95vi0nFbN","executionInfo":{"status":"ok","timestamp":1737998444592,"user_tz":-60,"elapsed":8,"user":{"displayName":"Thomas FERRATO","userId":"01314473185706532593"}}},"execution_count":19,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}